{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03479625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Video\n",
    "# Video(\"project 14 Coding your own custom Dense Layer.webm\",  embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14a1b6",
   "metadata": {},
   "source": [
    "# Ungraded Lab: Building a Custom Dense Layer\n",
    "\n",
    "In this lab, we'll walk through how to create a custom layer that inherits the [Layer](https://keras.io/api/layers/base_layer/#layer-class) class. Unlike simple Lambda layers you did previously, the custom layer here will contain weights that can be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2737220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97bbc5",
   "metadata": {},
   "source": [
    "## Custom Layer with weights\n",
    "\n",
    "To make custom layer that is trainable, we need to define a class that inherits the [Layer](https://keras.io/api/layers/base_layer/#layer-class) base class from Keras. The Python syntax is shown below in the class declaration. This class requires three functions: `__init__()`, `build()` and `call()`. These ensure that our custom layer has a *state* and *computation* that can be accessed during training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25ead69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from this base class\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SimpleDense(Layer):\n",
    "\n",
    "    def __init__(self, units=32):  # by default this layer will have 32 units in it \n",
    "        '''Initializes the instance attributes'''\n",
    "        super(SimpleDense, self).__init__() # some initialization of the super class must be done as well\n",
    "        self.units = units \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        # initialize the weights\n",
    "        w_init = tf.random_normal_initializer() # initialize nurons randomly using normal distribution\n",
    "        self.w = tf.Variable(name=\"kernel\",\n",
    "            initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                                 dtype='float32'),\n",
    "            trainable=True)\n",
    "\n",
    "        # initialize the biases\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(name=\"bias\",\n",
    "            initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3253fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'simple_dense_7/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.00520594]], dtype=float32)>, <tf.Variable 'simple_dense_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# declare an instance of the class\n",
    "my_dense = SimpleDense(units=1)\n",
    "\n",
    "# define an input and feed into the layer\n",
    "x = tf.ones((1, 1))\n",
    "y = my_dense(x)\n",
    "\n",
    "# parameters of the base Layer class like `variables` can be used\n",
    "print(my_dense.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset\n",
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "\n",
    "# use the Sequential API to build a model with our custom layer\n",
    "my_layer = SimpleDense(units=1)\n",
    "model = tf.keras.Sequential([my_layer])\n",
    "\n",
    "# configure and train the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "model.fit(xs, ys, epochs=500,verbose=0)\n",
    "\n",
    "# perform inference\n",
    "print(model.predict([10.0]))\n",
    "\n",
    "# see the updated state of the variables\n",
    "print(my_layer.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c9bee",
   "metadata": {},
   "source": [
    "# Activation in Custom Layers\n",
    "\n",
    "In this lab, we extend our knowledge of building custom layers by adding an activation parameter. The implementation is pretty straightforward as you'll see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c5b8c",
   "metadata": {},
   "source": [
    "## Adding an activation layer\n",
    "\n",
    "To use the built-in activations in Keras, we can specify an `activation` parameter in the `__init__()` method of our custom layer class. From there, we can initialize it by using the `tf.keras.activations.get()` method. This takes in a string identifier that corresponds to one of the [available activations](https://keras.io/api/layers/activations/#available-activations) in Keras. Next, you can now pass in the forward computation to this activation in the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0b0106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(Layer):\n",
    "\n",
    "    # add an activation parameter\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        # define the activation to get from the built-in activation layers in Keras\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(name=\"kernel\",\n",
    "            initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                                 dtype='float32'),\n",
    "            trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(name=\"bias\",\n",
    "            initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "            trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # pass the computation to the activation layer\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21023c6f",
   "metadata": {},
   "source": [
    "We can now pass in an activation parameter to our custom layer. The string identifier is mostly the same as the function name so 'relu' below will get `tf.keras.activations.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f568c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2966 - accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1434 - accuracy: 0.9570\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1072 - accuracy: 0.9672\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0876 - accuracy: 0.9727\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0772 - accuracy: 0.9756\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0708 - accuracy: 0.9791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07081466168165207, 0.9790999889373779]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    SimpleDense(128, activation='relu'), #or activation = tf.keras.activations.relu\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10079de3",
   "metadata": {},
   "source": [
    "# Implement a Quadratic Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f5d0b",
   "metadata": {},
   "source": [
    "build a custom quadratic layer which computes $y = ax^2 + bx + c$. Similar to the ungraded lab, this layer will be plugged into a model that will be trained on the MNIST dataset. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab51d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30ffc2",
   "metadata": {},
   "source": [
    "### Define the quadratic layer (TODO)\n",
    "Implement a simple quadratic layer. It has 3 state variables: $a$, $b$ and $c$. The computation returned is $ax^2 + bx + c$. Make sure it can also accept an activation function.\n",
    "\n",
    "#### `__init__`\n",
    "- call `super(my_fun, self)` to access the base class of `my_fun`, and call the `__init__()` function to initialize that base class.  In this case, `my_fun` is `SimpleQuadratic` and its base class is `Layer`.\n",
    "- self.units: set this using one of the function parameters.\n",
    "- self.activation: The function parameter `activation` will be passed in as a string.  To get the tensorflow object associated with the string, please use `tf.keras.activations.get()` \n",
    "\n",
    "\n",
    "#### `build`\n",
    "The following are suggested steps for writing your code.  If you prefer to use fewer lines to implement it, feel free to do so.  Either way, you'll want to set `self.a`, `self.b` and `self.c`.\n",
    "\n",
    "- a_init: set this to tensorflow's `random_normal_initializer()`\n",
    "- a_init_val: Use the `random_normal_initializer()` that you just created and invoke it, setting the `shape` and `dtype`.\n",
    "    - The `shape` of `a` should have its row dimension equal to the last dimension of `input_shape`, and its column dimension equal to the number of units in the layer.  \n",
    "    - This is because you'll be matrix multiplying x^2 * a, so the dimensions should be compatible.\n",
    "    - set the dtype to 'float32'\n",
    "- self.a: create a tensor using tf.Variable, setting the initial_value and set trainable to True.\n",
    "\n",
    "- b_init, b_init_val, and self.b: these will be set in the same way that you implemented a_init, a_init_val and self.a\n",
    "- c_init: set this to `tf.zeros_initializer`.\n",
    "- c_init_val: Set this by calling the tf.zeros_initializer that you just instantiated, and set the `shape` and `dtype`\n",
    "  - shape: This will be a vector equal to the number of units.  This expects a tuple, and remember that a tuple `(9,)` includes a comma.\n",
    "  - dtype: set to 'float32'.\n",
    "- self.c: create a tensor using tf.Variable, and set the parameters `initial_value` and `trainable`.\n",
    "\n",
    "#### `call`\n",
    "The following section performs the multiplication x^2*a + x*b + c.  The steps are broken down for clarity, but you can also perform this calculation in fewer lines if you prefer.\n",
    "- x_squared: use tf.math.square()\n",
    "- x_squared_times_a: use tf.matmul().  \n",
    "  - If you see an error saying `InvalidArgumentError: Matrix size-incompatible`, please check the order of the matrix multiplication to make sure that the matrix dimensions line up.\n",
    "- x_times_b: use tf.matmul().\n",
    "- x2a_plus_xb_plus_c: add the three terms together.\n",
    "- activated_x2a_plus_xb_plus_c: apply the class's `activation` to the sum of the three terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab44a2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomment all lines in this cell and replace those marked with `# YOUR CODE HERE`.\n",
    "# You can select all lines in this code cell with Ctrl+A (Windows/Linux) or Cmd+A (Mac), then press Ctrl+/ (Windows/Linux) or Cmd+/ (Mac) to uncomment.\n",
    "\n",
    "\n",
    "\n",
    "class SimpleQuadratic(Layer):\n",
    "\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        '''Initializes the class and sets up the internal variables'''\n",
    "        super(SimpleQuadratic , self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)   \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        '''Create the state of the layer (weights)'''\n",
    "        \n",
    "        a_init = tf.random_normal_initializer()\n",
    "        b_init = tf.random_normal_initializer()\n",
    "        c_init = tf.zeros_initializer()\n",
    "        \n",
    "        self.a = tf.Variable(name = \"kernel\", initial_value = a_init(shape= (input_shape[-1], self.units), \n",
    "                                                                    dtype= \"float32\"), trainable = True)\n",
    "        \n",
    "        self.b = tf.Variable(name = \"kernel\", initial_value = b_init(shape= (input_shape[-1], self.units), \n",
    "                                                                    dtype= \"float32\"), trainable = True)\n",
    "        \n",
    "        self.c = tf.Variable(name = \"bias\", initial_value = c_init(shape= (self.units,), \n",
    "                                                                    dtype= \"float32\"), trainable = True)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def call(self, inputs):\n",
    "        '''Defines the computation from inputs to outputs'''\n",
    "        result = tf.matmul(tf.math.square(inputs), self.a) + tf.matmul(inputs, self.b) + self.c\n",
    "        return self.activation(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5087ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2708 - accuracy: 0.9201\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1342 - accuracy: 0.9584\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1026 - accuracy: 0.9681\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0851 - accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0718 - accuracy: 0.9773\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0744 - accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0743996724486351, 0.9761000275611877]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS CODE SHOULD RUN WITHOUT MODIFICATION\n",
    "# AND SHOULD RETURN TRAINING/TESTING ACCURACY at 97%+\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  SimpleQuadratic(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
